{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA and Modelling - Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Import Libraries and Data](#chapterLibraryData)\n",
    "\n",
    "* [Chapter 1. Data Context ](#chapter1)\n",
    "* [Chapter 2. Data Description ](#chapter2)\n",
    "  * [Section 2.1 Data Loading](#section_2_1)\n",
    "  * [Section 2.2 Analysis 1 - Volumetric Analysis](#section_2_2)\n",
    "  * [Section 2.3 Analysis 2 - Univariate Analysis - NonGraphical / Graphical + Data Cleaning](#section_2_3)\n",
    "   * [Section 2.4 Analysis 3 - Bivariate Analysis - NonGraphical / Graphical](#section_2_4)\n",
    "   * [Section 2.5 Analysis 4 - Feature Selection](#section_2_5)\n",
    "   * [Section 2.6 Analysis 5 - Feature Encoding](#section_2_6)\n",
    "   * [Section 2.7 Analysis 6 - Modelling](#section_2_7)\n",
    "   * [Section 2.8 Analysis 7 - Fairness-Analysis](#section_2_8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries and Data <a class=\"anchor\" id=\"chapterLibraryData\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import warnings\n",
    "from IPython.display import display\n",
    "from dython.nominal import associations\n",
    "import missingno as msno\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import precision_score, recall_score, matthews_corrcoef\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "import warnings\n",
    "import logging\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import auc\n",
    "from scipy.stats import shapiro\n",
    "import joblib\n",
    "import plotly.offline as pyo\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from fairlearn.metrics import (MetricFrame,demographic_parity_difference, equalized_odds_difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Data Context\n",
    "\n",
    "The datasource of this case study is public and sourced from Home Credit (2018), a non-banking international financial institution providing lending services to individuals with little or no credit history. \n",
    "\n",
    "The datasource can be found on Kaggle (https://www.kaggle.com/competitions/home-credit-default-risk/overview), where it was featured in a past competition addressing the task of predicting applicants' repayment capability for a requested loan.\n",
    "\n",
    "The datasource consists of several tables, of which 2 primary ones were selected: application_train and bureau. \n",
    "\n",
    "1. application_train.csv\n",
    "\n",
    "- consists of 307.511 rows and 122 columns, which includes customer details - past loan characteristics and personal facts at the time of the application\n",
    "- \"TARGET\", serves as the target of the case study's prediction task, having two distinct values: 1 - the customer had repayment difficulties and 0 - the customer did not have repayment difficulties\n",
    "- \"SK_ID_CURR\" is a unique column used to identify past loans issued by Home Credit\n",
    "\n",
    "2. bureau.csv\n",
    "\n",
    "- consists of 1.716.428 rows and 17 columns, which include details from loans of Home Credit's customers granted by other financial institutions\n",
    "- \"SK_ID_BUREAU\", unique column used to identify the table's specific loans\n",
    "- \"SK_ID_CURR\" refers to the past loans issued by Home Credit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../figure/erd_picture.png\" alt=\"ERD\" width=\"340\" height=\"150\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "application = pd.read_csv(\"../data/raw/application_train.csv\")\n",
    "bureau = pd.read_csv(\"../data/raw/bureau.csv\")\n",
    "print(\"Shape application: \" + str(application.shape))\n",
    "print(\"Shape bureau: \" + str(bureau.shape))\n",
    "application['SK_ID_CURR'] = application['SK_ID_CURR'].astype('object')\n",
    "bureau['SK_ID_CURR'] = bureau['SK_ID_CURR'].astype('object') \n",
    "bureau['SK_ID_BUREAU'] = bureau['SK_ID_BUREAU'].astype('object') \n",
    "display(application.head())\n",
    "display(bureau.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_agg(x): \n",
    "    return x.mode().iloc[0] if (not x.mode().empty) and (not x.isnull().all()) else np.nan\n",
    "def numerical_agg(x): \n",
    "    return x.mean() if not x.isnull().all() else np.nan\n",
    "\n",
    "results = {}\n",
    "for column in bureau.select_dtypes(exclude=['int', 'float']).columns: \n",
    "    results[column] = bureau.groupby('SK_ID_CURR')[column].apply(categorical_agg) \n",
    "for column in bureau.select_dtypes(include=['int', 'float']).columns: \n",
    "    results[column] = bureau.groupby('SK_ID_CURR')[column].apply(numerical_agg) \n",
    "bureau = pd.DataFrame(results)\n",
    "bureau.reset_index(drop=True, inplace=True) \n",
    "\n",
    "data = application.merge(bureau, on='SK_ID_CURR', how='inner')\n",
    "\n",
    "flag_columns = [f'FLAG_DOCUMENT_{i}' for i in range(2, 22)] \n",
    "for col in flag_columns: \n",
    "    data[col] = data[col].astype('object') \n",
    "columns_to_convert = [ \n",
    "    'SK_ID_CURR', 'SK_ID_BUREAU', 'FLAG_MOBIL', \n",
    "    'FLAG_EMP_PHONE', 'FLAG_WORK_PHONE', 'FLAG_CONT_MOBILE',\n",
    "    'FLAG_PHONE', 'FLAG_EMAIL', 'REGION_RATING_CLIENT', 'REGION_RATING_CLIENT_W_CITY', 'REG_REGION_NOT_LIVE_REGION','REG_REGION_NOT_WORK_REGION',\n",
    "    'LIVE_REGION_NOT_WORK_REGION', 'REG_CITY_NOT_LIVE_CITY', 'REG_CITY_NOT_WORK_CITY', 'LIVE_CITY_NOT_WORK_CITY'\n",
    "]\n",
    "for col in columns_to_convert: \n",
    "    data[col] = data[col].astype('object')\n",
    "\n",
    "data.drop(columns=['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3'], inplace=True) \n",
    "flag_columns = [col for col in data.columns if col.startswith('FLAG_DOCUMENT_')]\n",
    "data['FD_TOTAL'] = data[flag_columns].sum(axis=1)\n",
    "columns_to_drop = [col for col in data.columns if col.startswith('FLAG_DOCUMENT_')]\n",
    "data.drop(columns=columns_to_drop, inplace=True) \n",
    "\n",
    "data.rename(columns={col: col+'_app' for col in application.columns if col != 'SK_ID_CURR'}, inplace=True) \n",
    "data.rename(columns={col: col+'_bur' for col in bureau.columns if col != 'SK_ID_CURR'}, inplace=True) \n",
    "\n",
    "print(\"Shape data: \" + str(data.shape))\n",
    "\n",
    "data.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis 1 - Volumetric Analysis: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section examines the “surface” properties of the acquired data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def volumetric_analysis(df):\n",
    "    print(\"Data Types:\\n\" + str(df.dtypes))\n",
    "    print(\"\\nRows and Columns: \\n\" + str(df.shape))\n",
    "    print(\"\\nColumns Names:\\n\" + str(df.columns))\n",
    "    \n",
    "    null_percentages = df.isnull().mean().sort_values(ascending=False)\n",
    "    print(\"\\nNull Values Percentage (Descending Order):\\n\" + str(null_percentages))\n",
    "    null_values_desc = df.isnull().sum().sort_values(ascending=False)\n",
    "    print(\"\\nNull Values (Descending Order):\\n\" + str(null_values_desc))\n",
    "    print(\"\\nDuplicate Values:\\n\" + str(df[df.duplicated()].shape))\n",
    "\n",
    "volumetric_analysis(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis 2 - Univariate Analysis - NonGraphical / Graphical + Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section analyses several columns of the dataset separately using descriptive statistics based on data type and visual preference: count of unique values, frequency distributions (ungrouped - using bar charts and grouped - using histograms), measures of center (mode, meadian, mode) and measures of dispersion (standard deviation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['TARGET_app'] = data['TARGET_app'].replace({0: 'non-default', 1: 'default'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Qualitative data - NonGraphical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_unique = []\n",
    "object_column_count = 0\n",
    "for col in data.select_dtypes(include='object').columns:\n",
    "    object_column_count += 1\n",
    "    data_unique.append([col, data[col].nunique()])\n",
    "\n",
    "df_cat_unique_values = pd.DataFrame(data_unique, columns=['Variable_name', 'Count of Unique values'])\n",
    "print(\"Number of object columns:\", object_column_count)\n",
    "print(df_cat_unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Frequency Distribution': data[\"TARGET_app\"].value_counts(),'Relative Frequency': data[\"TARGET_app\"].value_counts(normalize=True).mul(100).round(1).astype(str) + '%'})\n",
    "df.index.name = 'TARGET_app' \n",
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Frequency Distribution': data[\"NAME_CONTRACT_TYPE_app\"].value_counts(),'Relative Frequency': data[\"NAME_CONTRACT_TYPE_app\"].value_counts(normalize=True).mul(100).round(1).astype(str) + '%'})\n",
    "df.index.name = 'NAME_CONTRACT_TYPE_app' \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Frequency Distribution': data[\"CODE_GENDER_app\"].value_counts(),'Relative Frequency': data[\"CODE_GENDER_app\"].value_counts(normalize=True).mul(100).round(1).astype(str) + '%'})\n",
    "df.index.name = 'CODE_GENDER_app' \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Frequency Distribution': data[\"FLAG_OWN_CAR_app\"].value_counts(),'Relative Frequency': data[\"FLAG_OWN_CAR_app\"].value_counts(normalize=True).mul(100).round(1).astype(str) + '%'})\n",
    "df.index.name = 'FLAG_OWN_CAR_app' \n",
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Frequency Distribution': data[\"FLAG_OWN_REALTY_app\"].value_counts(),'Relative Frequency': data[\"FLAG_OWN_REALTY_app\"].value_counts(normalize=True).mul(100).round(1).astype(str) + '%'})\n",
    "df.index.name = 'FLAG_OWN_REALTY_app' \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Frequency Distribution': data[\"NAME_TYPE_SUITE_app\"].value_counts(),'Relative Frequency': data[\"NAME_TYPE_SUITE_app\"].value_counts(normalize=True).mul(100).round(1).astype(str) + '%'})\n",
    "df.index.name = 'NAME_TYPE_SUITE_app' \n",
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Frequency Distribution': data[\"NAME_INCOME_TYPE_app\"].value_counts(),'Relative Frequency': data[\"NAME_INCOME_TYPE_app\"].value_counts(normalize=True).mul(100).round(1).astype(str) + '%'})\n",
    "df.index.name = 'NAME_INCOME_TYPE_app' \n",
    "df.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Frequency Distribution': data[\"NAME_EDUCATION_TYPE_app\"].value_counts(),'Relative Frequency': data[\"NAME_EDUCATION_TYPE_app\"].value_counts(normalize=True).mul(100).round(1).astype(str) + '%'})\n",
    "df.index.name = 'NAME_EDUCATION_TYPE_app' \n",
    "df.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Frequency Distribution': data[\"NAME_FAMILY_STATUS_app\"].value_counts(),'Relative Frequency': data[\"NAME_FAMILY_STATUS_app\"].value_counts(normalize=True).mul(100).round(1).astype(str) + '%'})\n",
    "df.index.name = 'NAME_FAMILY_STATUS_app' \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Frequency Distribution': data[\"NAME_HOUSING_TYPE_app\"].value_counts(),'Relative Frequency': data[\"NAME_HOUSING_TYPE_app\"].value_counts(normalize=True).mul(100).round(1).astype(str) + '%'})\n",
    "df.index.name = 'NAME_HOUSING_TYPE_app' \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Frequency Distribution': data[\"FLAG_MOBIL_app\"].value_counts(),'Relative Frequency': data[\"FLAG_MOBIL_app\"].value_counts(normalize=True).mul(100).round(1).astype(str) + '%'})\n",
    "df.index.name = 'FLAG_MOBIL_app' \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Frequency Distribution': data[\"FLAG_EMP_PHONE_app\"].value_counts(),'Relative Frequency': data[\"FLAG_EMP_PHONE_app\"].value_counts(normalize=True).mul(100).round(1).astype(str) + '%'})\n",
    "df.index.name = 'FLAG_EMP_PHONE_app' \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Frequency Distribution': data[\"FLAG_WORK_PHONE_app\"].value_counts(),'Relative Frequency': data[\"FLAG_WORK_PHONE_app\"].value_counts(normalize=True).mul(100).round(1).astype(str) + '%'})\n",
    "df.index.name = 'FLAG_WORK_PHONE_app' \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Frequency Distribution': data[\"FLAG_CONT_MOBILE_app\"].value_counts(),'Relative Frequency': data[\"FLAG_CONT_MOBILE_app\"].value_counts(normalize=True).mul(100).round(1).astype(str) + '%'})\n",
    "df.index.name = 'FLAG_CONT_MOBILE_app' \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Frequency Distribution': data[\"FLAG_PHONE_app\"].value_counts(),'Relative Frequency': data[\"FLAG_PHONE_app\"].value_counts(normalize=True).mul(100).round(1).astype(str) + '%'})\n",
    "df.index.name = 'FLAG_PHONE_app' \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Frequency Distribution': data[\"FLAG_EMAIL_app\"].value_counts(),'Relative Frequency': data[\"FLAG_EMAIL_app\"].value_counts(normalize=True).mul(100).round(1).astype(str) + '%'})\n",
    "df.index.name = 'FLAG_EMAIL_app' \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Frequency Distribution': data[\"OCCUPATION_TYPE_app\"].value_counts(),'Relative Frequency': data[\"OCCUPATION_TYPE_app\"].value_counts(normalize=True).mul(100).round(1).astype(str) + '%'})\n",
    "df.index.name = 'OCCUPATION_TYPE_app' \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Frequency Distribution': data[\"REGION_RATING_CLIENT_app\"].value_counts(),'Relative Frequency': data[\"REGION_RATING_CLIENT_app\"].value_counts(normalize=True).mul(100).round(1).astype(str) + '%'})\n",
    "df.index.name = 'REGION_RATING_CLIENT_app' \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Frequency Distribution': data[\"REGION_RATING_CLIENT_W_CITY_app\"].value_counts(),'Relative Frequency': data[\"REGION_RATING_CLIENT_W_CITY_app\"].value_counts(normalize=True).mul(100).round(1).astype(str) + '%'})\n",
    "df.index.name = 'REGION_RATING_CLIENT_W_CITY_app' \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Frequency Distribution': data[\"WEEKDAY_APPR_PROCESS_START_app\"].value_counts(),'Relative Frequency': data[\"WEEKDAY_APPR_PROCESS_START_app\"].value_counts(normalize=True).mul(100).round(1).astype(str) + '%'})\n",
    "df.index.name = 'WEEKDAY_APPR_PROCESS_START_app' \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Frequency Distribution': data[\"REG_REGION_NOT_LIVE_REGION_app\"].value_counts(),'Relative Frequency': data[\"REG_REGION_NOT_LIVE_REGION_app\"].value_counts(normalize=True).mul(100).round(1).astype(str) + '%'})\n",
    "df.index.name = 'REG_REGION_NOT_LIVE_REGION_app' \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Frequency Distribution': data[\"REG_REGION_NOT_WORK_REGION_app\"].value_counts(),'Relative Frequency': data[\"REG_REGION_NOT_WORK_REGION_app\"].value_counts(normalize=True).mul(100).round(1).astype(str) + '%'})\n",
    "df.index.name = 'REG_REGION_NOT_WORK_REGION_app' \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Frequency Distribution': data[\"LIVE_REGION_NOT_WORK_REGION_app\"].value_counts(),'Relative Frequency': data[\"LIVE_REGION_NOT_WORK_REGION_app\"].value_counts(normalize=True).mul(100).round(1).astype(str) + '%'})\n",
    "df.index.name = 'LIVE_REGION_NOT_WORK_REGION_app' \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Frequency Distribution': data[\"REG_CITY_NOT_LIVE_CITY_app\"].value_counts(),'Relative Frequency': data[\"REG_CITY_NOT_LIVE_CITY_app\"].value_counts(normalize=True).mul(100).round(1).astype(str) + '%'})\n",
    "df.index.name = 'REG_CITY_NOT_LIVE_CITY_app' \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Frequency Distribution': data[\"REG_CITY_NOT_WORK_CITY_app\"].value_counts(),'Relative Frequency': data[\"REG_CITY_NOT_WORK_CITY_app\"].value_counts(normalize=True).mul(100).round(1).astype(str) + '%'})\n",
    "df.index.name = 'REG_CITY_NOT_WORK_CITY_app' \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Frequency Distribution': data[\"LIVE_CITY_NOT_WORK_CITY_app\"].value_counts(),'Relative Frequency': data[\"LIVE_CITY_NOT_WORK_CITY_app\"].value_counts(normalize=True).mul(100).round(1).astype(str) + '%'})\n",
    "df.index.name = 'LIVE_CITY_NOT_WORK_CITY_app' \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Frequency Distribution': data[\"ORGANIZATION_TYPE_app\"].value_counts(),'Relative Frequency': data[\"ORGANIZATION_TYPE_app\"].value_counts(normalize=True).mul(100).round(1).astype(str) + '%'})\n",
    "df.index.name = 'ORGANIZATION_TYPE_app' \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Frequency Distribution': data[\"FONDKAPREMONT_MODE_app\"].value_counts(),'Relative Frequency': data[\"FONDKAPREMONT_MODE_app\"].value_counts(normalize=True).mul(100).round(1).astype(str) + '%'})\n",
    "df.index.name = 'FONDKAPREMONT_MODE_app' \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Frequency Distribution': data[\"HOUSETYPE_MODE_app\"].value_counts(),'Relative Frequency': data[\"HOUSETYPE_MODE_app\"].value_counts(normalize=True).mul(100).round(1).astype(str) + '%'})\n",
    "df.index.name = 'HOUSETYPE_MODE_app' \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Frequency Distribution': data[\"WALLSMATERIAL_MODE_app\"].value_counts(),'Relative Frequency': data[\"WALLSMATERIAL_MODE_app\"].value_counts(normalize=True).mul(100).round(1).astype(str) + '%'})\n",
    "df.index.name = 'WALLSMATERIAL_MODE_app' \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Frequency Distribution': data[\"EMERGENCYSTATE_MODE_app\"].value_counts(),'Relative Frequency': data[\"EMERGENCYSTATE_MODE_app\"].value_counts(normalize=True).mul(100).round(1).astype(str) + '%'})\n",
    "df.index.name = 'EMERGENCYSTATE_MODE_app' \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Frequency Distribution': data[\"CREDIT_ACTIVE_bur\"].value_counts(),'Relative Frequency': data[\"CREDIT_ACTIVE_bur\"].value_counts(normalize=True).mul(100).round(1).astype(str) + '%'})\n",
    "df.index.name = 'CREDIT_ACTIVE_bur' \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Frequency Distribution': data[\"CREDIT_CURRENCY_bur\"].value_counts(),'Relative Frequency': data[\"CREDIT_CURRENCY_bur\"].value_counts(normalize=True).mul(100).round(1).astype(str) + '%'})\n",
    "df.index.name = 'CREDIT_CURRENCY_bur' \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Frequency Distribution': data[\"CREDIT_TYPE_bur\"].value_counts(),'Relative Frequency': data[\"CREDIT_TYPE_bur\"].value_counts(normalize=True).mul(100).round(1).astype(str) + '%'})\n",
    "df.index.name = 'CREDIT_TYPE_bur' \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Qualitative data - Graphical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_counts = data[\"TARGET_app\"].value_counts(normalize=True)\n",
    "set3_palette = px.colors.qualitative.Set3\n",
    "colors = [set3_palette[4], set3_palette[5]]  \n",
    "\n",
    "fig = px.bar(x=value_counts.index, \n",
    "             y=value_counts * 100, \n",
    "             text=(value_counts * 100).round(2).astype(str) + '%', \n",
    "             color=value_counts.index,\n",
    "             color_discrete_sequence=colors,\n",
    "             height=750, \n",
    "             width=600, \n",
    "             title='% Distribution of TARGET')\n",
    "\n",
    "fig.update_traces(textfont_size=12, textposition=\"inside\")\n",
    "fig.update_layout(yaxis_title='Percentage', xaxis_title='TARGET',xaxis_title_standoff=377)\n",
    "fig.update_xaxes(tickvals=['No', 'Yes'], ticktext=['No', 'Yes'])\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Qualitative data - Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['CODE_GENDER_app'] != 'XNA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_type_split(data):\n",
    "    cat_list = []\n",
    "    dis_num_list = []\n",
    "    num_list = []\n",
    "    for i in data.columns.tolist():\n",
    "        if data[i].dtype == 'object':\n",
    "            cat_list.append(i)\n",
    "        elif data[i].nunique() < 25:\n",
    "            dis_num_list.append(i)\n",
    "        else:\n",
    "            num_list.append(i)\n",
    "    return cat_list, dis_num_list, num_list\n",
    "\n",
    "cat_list, dis_num_list, num_list = feature_type_split(data) \n",
    "print(str(len(cat_list)),'categorical features:', cat_list)\n",
    "print('-----------------------------------------')\n",
    "print(str(len(dis_num_list)),'discrete numerical features:',dis_num_list)\n",
    "print('-----------------------------------------')\n",
    "print(str(len(num_list)),'continuous numerical features:',num_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_list, _, _ = feature_type_split(data)\n",
    "for col in cat_list:\n",
    "    data[col] = data[col].apply(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "\n",
    "data.replace('', np.nan, inplace=True)\n",
    "null_percentages = (data[cat_list].isnull().sum() / len(data)) * 100\n",
    "\n",
    "print(\"Null percentages:\")\n",
    "print(null_percentages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.bar(data[cat_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 8))\n",
    "colours = ['#34495E', 'seagreen']\n",
    "sns.heatmap(data[cat_list].isnull(), cmap=sns.color_palette(colours))\n",
    "plt.title('Heatmap of Missing Values for Object Columns')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. NAME_TYPE_SUITE_app (MCAR) : no pattern / correlation with its own column or other columns (heatmaps and bar charts) => Complete case analysis\n",
    "2. OCCUPATION_TYPE_app (MCAR) : no pattern / correlation with its own column or other columns (heatmaps and bar charts) => Complete case analysis => considerable amount of rows => Column detetion\n",
    "3. FONDKAPREMONT_MODE_app, HOUSETYPE_MODE_app, WALLSMATERIAL_MODE_app, EMERGENCYSTATE_MODE_app (MAR) : pattern / correlation between the columns => Single imputation methods not valid => Multiple imputation needed => less directly applicable to categorical data than on numerical data => Column deletion\n",
    "4. SK_ID_BUREAU_bur (MAR) : due to initial aggregation performed in order to do the grouping => Single imputation value (\"missing\") as we only use the variable for feature engineering purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(subset=['NAME_TYPE_SUITE_app'])\n",
    "columns_to_remove = ['FONDKAPREMONT_MODE_app', 'HOUSETYPE_MODE_app', 'WALLSMATERIAL_MODE_app', 'EMERGENCYSTATE_MODE_app','OCCUPATION_TYPE_app']\n",
    "data = data.drop(columns=columns_to_remove)\n",
    "data['SK_ID_BUREAU_bur'] = data['SK_ID_BUREAU_bur'].fillna('missing')\n",
    "print(\"Shape \" + str(data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_list, dis_num_list, num_list = feature_type_split(data) \n",
    "print(str(len(cat_list)),'categorical features:', cat_list)\n",
    "print('-----------------------------------------')\n",
    "print(str(len(dis_num_list)),'discrete numerical features:',dis_num_list)\n",
    "print('-----------------------------------------')\n",
    "print(str(len(num_list)),'continuous numerical features:',num_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 8))\n",
    "colours = [ 'seagreen','#34495E'] \n",
    "sns.heatmap(data[cat_list].isnull(), cmap=sns.color_palette(colours))\n",
    "plt.title('Heatmap of Missing Values for Object Columns')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Quantitative data - NonGraphical / Graphical - Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"DAYS_BIRTH_app\"] = abs(data[\"DAYS_BIRTH_app\"])\n",
    "data[\"DAYS_BIRTH_app\"] = data[\"DAYS_BIRTH_app\"] / 365\n",
    "\n",
    "fig = px.histogram(data, x='DAYS_BIRTH_app', title='Histogram of DAYS_BIRTH', width=800)\n",
    "median_value = data['DAYS_BIRTH_app'].median()\n",
    "mean_value = data['DAYS_BIRTH_app'].mean()\n",
    "mode_value = data['DAYS_BIRTH_app'].mode()[0]\n",
    "\n",
    "fig.add_vline(x=median_value, line_dash=\"dash\", line_color=\"green\", annotation_text=\"Median\",annotation_font=dict(size=4))\n",
    "fig.add_vline(x=mean_value, line_dash=\"dash\", line_color=\"blue\", annotation_text=\"Mean\", annotation_font=dict(size=4))\n",
    "fig.add_vline(x=mode_value, line_dash=\"dash\", line_color=\"red\", annotation_text=\"Mode\", annotation_font=dict(size=4))\n",
    "\n",
    "fig.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in data.select_dtypes(include=['number']).columns:\n",
    "    data[column] = data[column].apply(lambda x: abs(x) if x != 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_list, dis_num_list, num_list = feature_type_split(data)\n",
    "for col in dis_num_list + num_list:\n",
    "    if data[col].dtype == 'object':\n",
    "        data[col] = data[col].apply(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "\n",
    "data.replace('', np.nan, inplace=True)\n",
    "\n",
    "null_percentages_dis_num = (data[dis_num_list].isnull().sum() / len(data)) * 100\n",
    "print(\"Null percentages for discrete numerical features:\")\n",
    "print(null_percentages_dis_num)\n",
    "print('-----------------------------------------')\n",
    "\n",
    "null_percentages_num = (data[num_list].isnull().sum() / len(data)) * 100\n",
    "print(\"Null percentages for continuous numerical features:\")\n",
    "print(null_percentages_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_convert = [ \n",
    "    'SK_ID_CURR', 'SK_ID_BUREAU_bur', 'FLAG_MOBIL_app', \n",
    "    'FLAG_EMP_PHONE_app', 'FLAG_WORK_PHONE_app', 'FLAG_CONT_MOBILE_app',\n",
    "    'FLAG_PHONE_app', 'FLAG_EMAIL_app', 'REGION_RATING_CLIENT_app', 'REGION_RATING_CLIENT_W_CITY_app', 'REG_REGION_NOT_LIVE_REGION_app','REG_REGION_NOT_WORK_REGION_app',\n",
    "    'LIVE_REGION_NOT_WORK_REGION_app', 'REG_CITY_NOT_LIVE_CITY_app', 'REG_CITY_NOT_WORK_CITY_app', 'LIVE_CITY_NOT_WORK_CITY_app'\n",
    "]\n",
    "for col in columns_to_convert: \n",
    "    data[col] = data[col].astype('object')\n",
    "\n",
    "cat_list, dis_num_list, num_list = feature_type_split(data)\n",
    "\n",
    "combined_list = dis_num_list + num_list\n",
    "msno.bar(data[combined_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_data = data[combined_list]\n",
    "plt.figure(figsize=(25, 8))\n",
    "colours = ['#34495E', 'seagreen']\n",
    "sns.heatmap(numerical_data.isnull(), cmap=sns.color_palette(colours))\n",
    "plt.title('Heatmap of Missing Values for Numerical Columns')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. DAYS_ENDDATE_FACT_bur, AMT_CREDIT_SUM_DEBT_bur, AMT_CREDIT_SUM_LIMIT_bur, DAYS_CREDIT_ENDDATE_bur (MCAR) => no pattern / correlation with its own column or other columns (heatmaps and bar charts) => Complete case analysis\n",
    "2. OWN_CAR_AGE_app, AMT_CREDIT_MAX_OVERDUE_bur, AMT_ANNUITY_y (MCAR) => no pattern / correlation with its own column or other columns (heatmaps and bar charts) => Complete case analysis => considerable amount of rows => Column deletion\n",
    "3. Other columns (MAR) => pattern / correlation between the columns => Single imputation methods not valid => Multiple imputation needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(subset=['DAYS_ENDDATE_FACT_bur','AMT_CREDIT_SUM_DEBT_bur','AMT_CREDIT_SUM_LIMIT_bur','DAYS_CREDIT_ENDDATE_bur'])\n",
    "columns_to_remove = ['OWN_CAR_AGE_app','AMT_CREDIT_MAX_OVERDUE_bur','AMT_ANNUITY_y']\n",
    "data = data.drop(columns=columns_to_remove)\n",
    "columns_with_missing_values = data.columns[data.isnull().any()].tolist()\n",
    "imputer = IterativeImputer()\n",
    "data[columns_with_missing_values] = imputer.fit_transform(data[columns_with_missing_values])\n",
    "print(\"Shape \" + str(data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_cols = []\n",
    "\n",
    "for column in data.select_dtypes(include=['float64', 'int64']).columns:\n",
    "       \n",
    "    stat, p_value = shapiro(data[column])  # Perform Shapiro-Wilk test\n",
    "    if p_value >= 0.05:\n",
    "        normal_cols.append(column)\n",
    "\n",
    "print(\"Columns with normal distribution:\")\n",
    "print(normal_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_list, dis_num_list, num_list = feature_type_split(data) \n",
    "print(str(len(cat_list)),'categorical features:', cat_list)\n",
    "print('-----------------------------------------')\n",
    "print(str(len(dis_num_list)),'discrete numerical features:',dis_num_list)\n",
    "print('-----------------------------------------')\n",
    "print(str(len(num_list)),'continuous numerical features:',num_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_list = dis_num_list + num_list\n",
    "numerical_data = data[combined_list]\n",
    "plt.figure(figsize=(25, 8))\n",
    "colours = ['seagreen','#34495E' ]\n",
    "sns.heatmap(numerical_data.isnull(), cmap=sns.color_palette(colours))\n",
    "plt.title('Heatmap of Missing Values for Numerical Columns')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_outliers_iqr(column):\n",
    "    Q1 = column.quantile(0.25)\n",
    "    Q3 = column.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = column[(column < lower_bound) | (column > upper_bound)]\n",
    "    return outliers\n",
    "\n",
    "print(\"Shape before removing outliers:\", data.shape)\n",
    "\n",
    "for column in num_list:\n",
    "    outliers = identify_outliers_iqr(data[column])\n",
    "    data = data[~data[column].isin(outliers)]\n",
    "\n",
    "print(\"Shape after removing outliers 1:\", data.shape)\n",
    "\n",
    "for column in dis_num_list:\n",
    "    outliers = identify_outliers_iqr(data[column])\n",
    "    data = data[~data[column].isin(outliers)]\n",
    "\n",
    "print(\"Shape after removing outliers 2:\", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "for column in num_list:\n",
    "    data[[column]] = scaler.fit_transform(data[[column]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis 3 - Bivariate Analysis - NonGraphical / Graphical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the imbalance nature of the target variable, we assume that we cannot visually observe any clear patterns between \"TARGET_app\" and the other variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis 4 - Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Correlation-based feature selection - Quantitative data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['TARGET_app'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['TARGET_app'] = data['TARGET_app'].replace({'non-default': 0, 'default': 1})\n",
    "numeric_columns_df = data.select_dtypes(include=['number'])\n",
    "numerical_corr_spearman = numeric_columns_df.corr(method='spearman')\n",
    "numerical_corr_spearman = numerical_corr_spearman.style.map(lambda x: 'background-color: green' if abs(float(x)) > 0.80 else '') # Spearman is used because we assume that most numerical columns do not follow a normal distribution\n",
    "numerical_corr_spearman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns_df = data.select_dtypes(include=['number'])\n",
    "numerical_corr_spearman = numeric_columns_df.corr(method='spearman')\n",
    "sorted_corr = numerical_corr_spearman['TARGET_app'].abs().sort_values(ascending=False)\n",
    "top_10_features = sorted_corr.head(11)[1:]  # Excluding 'TARGET'\n",
    "\n",
    "print(\"Top 10 Numerical Features Correlated with 'TARGET':\")\n",
    "print(top_10_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns_df = data.select_dtypes(include=['number'])\n",
    "numerical_corr_spearman = numeric_columns_df.corr(method='spearman')\n",
    "high_corr_pairs = (numerical_corr_spearman.abs() > 0.80) & (numerical_corr_spearman.abs() < 1)\n",
    "\n",
    "columns_to_delete = set()\n",
    "\n",
    "for col1 in high_corr_pairs.columns:\n",
    "    for col2 in high_corr_pairs.index:\n",
    "        if high_corr_pairs.loc[col2, col1]:\n",
    "            if abs(numerical_corr_spearman.loc[col1, 'TARGET_app']) > abs(numerical_corr_spearman.loc[col2, 'TARGET_app']):\n",
    "                columns_to_delete.add(col2)\n",
    "            else:\n",
    "                columns_to_delete.add(col1)\n",
    "\n",
    "columns_removed = list(columns_to_delete)\n",
    "data.drop(columns=columns_to_delete, inplace=True)\n",
    "\n",
    "print(\"Columns Removed:\")\n",
    "for col in columns_removed:\n",
    "    print(col)\n",
    "\n",
    "print(\"\\nUpdated DataFrame:\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Removing features with low variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = data.select_dtypes(exclude=['object']).columns\n",
    "selector = VarianceThreshold(threshold=0) # constant\n",
    "selected_features = selector.fit_transform(data[numerical_cols])\n",
    "support = selector.get_support()\n",
    "variances = selector.variances_\n",
    "cols_to_remove = numerical_cols[~support]\n",
    "data = data.drop(columns=cols_to_remove)\n",
    "print(\"Numerical Columns to Remove based on Variance Threshold:\")\n",
    "print(cols_to_remove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Correlation-based feature selection - Qualitative data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_list, dis_num_list, num_list = feature_type_split(data) \n",
    "print(str(len(cat_list)),'categorical features:', cat_list)\n",
    "print('-----------------------------------------')\n",
    "print(str(len(dis_num_list)),'discrete numerical features:',dis_num_list)\n",
    "print('-----------------------------------------')\n",
    "print(str(len(num_list)),'continuous numerical features:',num_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_categorical = data[cat_list]\n",
    "corr_measures = associations(df_categorical, nominal_columns='all', plot=False)\n",
    "fig = px.imshow(corr_measures['corr'], color_continuous_scale=\"RdBu\", text_auto=True)\n",
    "fig.update_layout(title=\"Cramer's V Correlation Heatmap\", width=800, height=700)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data['LIVE_REGION_NOT_WORK_REGION_app']\n",
    "del data['REG_CITY_NOT_WORK_CITY_app']\n",
    "del data['REGION_RATING_CLIENT_W_CITY_app']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_list, dis_num_list, num_list = feature_type_split(data) \n",
    "print(str(len(cat_list)),'categorical features:', cat_list)\n",
    "print('-----------------------------------------')\n",
    "print(str(len(dis_num_list)),'discrete numerical features:',dis_num_list)\n",
    "print('-----------------------------------------')\n",
    "print(str(len(num_list)),'continuous numerical features:',num_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data['SK_ID_BUREAU_bur']\n",
    "data.set_index('SK_ID_CURR', inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_list = list(data.columns)\n",
    "print(columns_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis 5 - Feature Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le_count = 0\n",
    "le_encoded_cols = []  \n",
    "\n",
    "for col in data.columns[1:]:\n",
    "    if data[col].dtype == 'object':\n",
    "        if len(list(data[col].unique())) <= 2:\n",
    "            le.fit(data[col])\n",
    "            data[col] = le.transform(data[col])\n",
    "            le_count += 1\n",
    "            le_encoded_cols.append(col) \n",
    "\n",
    "print('{} columns were label encoded.'.format(le_count))\n",
    "print('Columns label encoded: {}'.format(', '.join(le_encoded_cols))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_list, dis_num_list, num_list = feature_type_split(data) \n",
    "print(str(len(cat_list)),'categorical features:', cat_list)\n",
    "print('-----------------------------------------')\n",
    "print(str(len(dis_num_list)),'discrete numerical features:',dis_num_list)\n",
    "print('-----------------------------------------')\n",
    "print(str(len(num_list)),'continuous numerical features:',num_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_one_hot_encode = [feature for feature in cat_list if data[feature].nunique() > 2]\n",
    "\n",
    "one_hot_df = pd.get_dummies(data[features_to_one_hot_encode])\n",
    "one_hot_df = one_hot_df.astype(int)\n",
    "\n",
    "data = pd.concat([data.drop(columns=[*features_to_one_hot_encode]), one_hot_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['TARGET_app'] = le.fit_transform(data['TARGET_app'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_list, dis_num_list, num_list = feature_type_split(data) \n",
    "print(str(len(cat_list)),'categorical features:', cat_list)\n",
    "print('-----------------------------------------')\n",
    "print(str(len(dis_num_list)),'discrete numerical features:',dis_num_list)\n",
    "print('-----------------------------------------')\n",
    "print(str(len(num_list)),'continuous numerical features:',num_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis 6 - Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['TARGET_app']\n",
    "X = data.drop(columns=['TARGET_app'])\n",
    "X_train, X_remaining, y_train, y_remaining = train_test_split(X, y, test_size=0.2, stratify=y, random_state=8) # X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2, stratify=y, random_state = 8)\n",
    "X_test, X_calibration, y_test, y_calibration = train_test_split(X_remaining, y_remaining, test_size=0.5, stratify=y_remaining, random_state=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_remaining.to_csv('X_remaining_cp.csv', index=True)\n",
    "y_remaining_df = pd.DataFrame({'TARGET_app': y_remaining})\n",
    "# y_remaining_df.to_csv('y_remaining_cp.csv', index=True)\n",
    "\n",
    "# X_test.to_csv('X_test_cp.csv', index=True)\n",
    "y_test_df = pd.DataFrame({'TARGET_app': y_test})\n",
    "# y_test_df.to_csv('y_test_cp.csv', index=True)\n",
    "# X_calibration.to_csv('X_calibration_cp.csv', index=True)\n",
    "y_calibration_df = pd.DataFrame({'TARGET_app': y_calibration})\n",
    "# y_calibration_df.to_csv('y_calibration_cp.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(random_state=8)\n",
    "X_train, y_train= smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print(f\"Resampled training features shape: {X_train.shape}\")\n",
    "print(f\"Resampled training target shape: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=10)\n",
    "\n",
    "def calculate_mcc(y_true, y_pred):\n",
    "    return matthews_corrcoef(y_true, y_pred) if len(np.unique(y_true)) > 1 else 0.0\n",
    "\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "mcc_scores = []\n",
    "\n",
    "for train, val in cv.split(X_train, y_train):\n",
    "\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train], X_train.iloc[val]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train], y_train.iloc[val]\n",
    "\n",
    "    model_x = RandomForestClassifier(random_state=42)\n",
    "    model_x.fit(X_train_fold, y_train_fold)\n",
    "    y_pred_val_fold = model_x.predict(X_val_fold)\n",
    "    \n",
    "    precision_fold = precision_score(y_val_fold, y_pred_val_fold)\n",
    "    precision_scores.append(precision_fold)\n",
    "    \n",
    "    recall_fold = recall_score(y_val_fold, y_pred_val_fold)\n",
    "    recall_scores.append(recall_fold)\n",
    "    \n",
    "    mcc_fold = calculate_mcc(y_val_fold, y_pred_val_fold)\n",
    "    mcc_scores.append(mcc_fold)\n",
    "\n",
    "average_precision_score = np.mean(precision_scores)\n",
    "average_recall_score = np.mean(recall_scores)\n",
    "average_mcc_score = np.mean(mcc_scores)\n",
    "\n",
    "print(\"Average precision: {:.4f}\".format(average_precision_score))\n",
    "print(\"Average recall: {:.4f}\".format(average_recall_score))\n",
    "print(\"Average MCC: {:.4f}\".format(average_mcc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=10)\n",
    "\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "mcc_scores = []\n",
    "\n",
    "for train, val in cv.split(X_train, y_train):\n",
    "\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train], X_train.iloc[val]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train], y_train.iloc[val]\n",
    "\n",
    "    model_x = XGBClassifier(random_state=42)\n",
    "    model_x.fit(X_train_fold, y_train_fold)\n",
    "    y_pred_val_fold = model_x.predict(X_val_fold)\n",
    "    \n",
    "    precision_fold = precision_score(y_val_fold, y_pred_val_fold)\n",
    "    precision_scores.append(precision_fold)\n",
    "    \n",
    "    recall_fold = recall_score(y_val_fold, y_pred_val_fold)\n",
    "    recall_scores.append(recall_fold)\n",
    "    \n",
    "    mcc_fold = calculate_mcc(y_val_fold, y_pred_val_fold)\n",
    "    mcc_scores.append(mcc_fold)\n",
    "\n",
    "average_precision_score = np.mean(precision_scores)\n",
    "average_recall_score = np.mean(recall_scores)\n",
    "average_mcc_score = np.mean(mcc_scores)\n",
    "\n",
    "print(\"Average precision: {:.4f}\".format(average_precision_score))\n",
    "print(\"Average recall: {:.4f}\".format(average_recall_score))\n",
    "print(\"Average MCC: {:.4f}\".format(average_mcc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns = [\"\".join(c if c.isalnum() else \"_\" for c in str(x)) for x in X_train.columns]\n",
    "\n",
    "cv = KFold(n_splits=10)\n",
    "\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "mcc_scores = []\n",
    "\n",
    "for train, val in cv.split(X_train, y_train):\n",
    "\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train], X_train.iloc[val]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train], y_train.iloc[val]\n",
    "\n",
    "    model_lgb = LGBMClassifier(random_state=42, verbose = 0)\n",
    "    model_lgb.fit(X_train_fold, y_train_fold)\n",
    "    y_pred_val_fold = model_lgb.predict(X_val_fold)\n",
    "    \n",
    "    precision_fold = precision_score(y_val_fold, y_pred_val_fold)\n",
    "    precision_scores.append(precision_fold)\n",
    "    \n",
    "    recall_fold = recall_score(y_val_fold, y_pred_val_fold)\n",
    "    recall_scores.append(recall_fold)\n",
    "    \n",
    "    mcc_fold = calculate_mcc(y_val_fold, y_pred_val_fold)\n",
    "    mcc_scores.append(mcc_fold)\n",
    "\n",
    "average_precision_score = np.mean(precision_scores)\n",
    "average_recall_score = np.mean(recall_scores)\n",
    "average_mcc_score = np.mean(mcc_scores)\n",
    "\n",
    "print(\"Average precision: {:.4f}\".format(average_precision_score))\n",
    "print(\"Average recall: {:.4f}\".format(average_recall_score))\n",
    "print(\"Average MCC: {:.4f}\".format(average_mcc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = make_scorer(matthews_corrcoef)\n",
    "\n",
    "param_grid = {'randomforestclassifier__n_estimators': [50, 100, 200],\n",
    "              'randomforestclassifier__max_depth': [10, 20, 30, 40, 50],\n",
    "              'randomforestclassifier__min_samples_split': [2, 5, 10],\n",
    "              'randomforestclassifier__min_samples_leaf': [1, 2, 4]} \n",
    "\n",
    "k = 10\n",
    "\n",
    "cv = KFold(n_splits=k)\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "pipeline = Pipeline([('randomforestclassifier', rf)])\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=cv, scoring=scorer)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best hyperparameters: \", grid_search.best_params_)\n",
    "print(\"Best MCC score: \", grid_search.best_score_)\n",
    "\n",
    "# Best hyperparameters:  {'randomforestclassifier__max_depth': 50, 'randomforestclassifier__min_samples_leaf': 1, 'randomforestclassifier__min_samples_split': 2, 'randomforestclassifier__n_estimators': 200}\n",
    "# Best MCC score:  0.33353458327828533"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = make_scorer(matthews_corrcoef)\n",
    "\n",
    "param_grid = {'xgbclassifier__n_estimators': [50, 100, 200],\n",
    "              'xgbclassifier__max_depth': [3, 4, 5, 6],\n",
    "              'xgbclassifier__learning_rate': [0.01, 0.05, 0.1]} \n",
    "\n",
    "k = 10\n",
    "cv = KFold(n_splits=k)\n",
    "\n",
    "xgb = XGBClassifier(random_state=42)\n",
    "\n",
    "pipeline = Pipeline([('xgbclassifier', xgb)])\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=cv, scoring=scorer)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best hyperparameters: \", grid_search.best_params_)\n",
    "print(\"Best MCC score: \", grid_search.best_score_)\n",
    "\n",
    "# Best hyperparameters:  {'xgbclassifier__learning_rate': 0.05, 'xgbclassifier__max_depth': 6, 'xgbclassifier__n_estimators': 200}\n",
    "# Best MCC score:  0.15417460213744283"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger('lightgbm').setLevel(logging.ERROR)\n",
    "scorer = make_scorer(matthews_corrcoef)\n",
    "\n",
    "param_grid = {'lgbmclassifier__n_estimators': [50, 100, 200],\n",
    "              'lgbmclassifier__max_depth': [3, 4, 5, 6],\n",
    "              'lgbmclassifier__learning_rate': [0.01, 0.05, 0.1]} \n",
    "\n",
    "k = 10\n",
    "cv = KFold(n_splits=k)\n",
    "\n",
    "lgbm = LGBMClassifier(random_state=42, verbose=0)\n",
    "pipeline = Pipeline([('lgbmclassifier', lgbm)])\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=cv, scoring=scorer)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best hyperparameters: \", grid_search.best_params_)\n",
    "print(\"Best MCC score: \", grid_search.best_score_)\n",
    "\n",
    "#Best hyperparameters:  {'lgbmclassifier__learning_rate': 0.1, 'lgbmclassifier__max_depth': 5, 'lgbmclassifier__n_estimators': 100}\n",
    "#Best MCC score:  0.14678247564181304"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'max_depth': 50,\n",
    "    'min_samples_leaf': 1,\n",
    "    'min_samples_split': 2,\n",
    "    'n_estimators': 200,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "def calculate_mcc(y_true, y_pred):\n",
    "    return matthews_corrcoef(y_true, y_pred) if len(np.unique(y_true)) > 1 else 0.0\n",
    "\n",
    "cv = KFold(n_splits=10)\n",
    "\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "mcc_scores = []\n",
    "\n",
    "for train, val in cv.split(X_train, y_train):\n",
    "\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train], X_train.iloc[val]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train], y_train.iloc[val]\n",
    "\n",
    "    model_rf = RandomForestClassifier(**params)\n",
    "    model_rf.fit(X_train_fold, y_train_fold)\n",
    "    y_pred_val_fold = model_rf.predict(X_val_fold)\n",
    "    \n",
    "    precision_fold = precision_score(y_val_fold, y_pred_val_fold)\n",
    "    precision_scores.append(precision_fold)\n",
    "    \n",
    "    recall_fold = recall_score(y_val_fold, y_pred_val_fold)\n",
    "    recall_scores.append(recall_fold)\n",
    "    \n",
    "    mcc_fold = calculate_mcc(y_val_fold, y_pred_val_fold)\n",
    "    mcc_scores.append(mcc_fold)\n",
    "\n",
    "average_precision_score = np.mean(precision_scores)\n",
    "average_recall_score = np.mean(recall_scores)\n",
    "average_mcc_score = np.mean(mcc_scores)\n",
    "\n",
    "print(\"Average precision: {:.4f}\".format(average_precision_score))\n",
    "print(\"Average recall: {:.4f}\".format(average_recall_score))\n",
    "print(\"Average MCC: {:.4f}\".format(average_mcc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf.fit(X_train, y_train)\n",
    "# joblib.dump(model_rf, 'model_rf.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_rf.predict(X_test)\n",
    "y_prob = model_rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "auc_score = roc_auc_score(y_test, y_prob)\n",
    "print(f\"AUC Score: {auc_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm_df = pd.DataFrame(cm, columns=['Predicted Negative', 'Predicted Positive'], index=['Actual Negative', 'Actual Positive'])\n",
    "\n",
    "fig = px.imshow(cm_df, labels=dict(x=\"Predicted\", y=\"Actual\", color=\"Count\"),\n",
    "                x=['Predicted Negative', 'Predicted Positive'],\n",
    "                y=['Actual Negative', 'Actual Positive'],\n",
    "                color_continuous_scale=px.colors.sequential.Blues,text_auto=True)\n",
    "\n",
    "fig.update_traces(showscale=True) \n",
    "fig.update_layout(title='Confusion Matrix')\n",
    "fig.show()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='blue', lw=1, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='red', lw=1, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis 7 - Fairness Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = X_test[\"CODE_GENDER_app\"]\n",
    "A_str = A.map({ 1:\"male\", 0:\"female\"})\n",
    "\n",
    "metrics_dict = {\n",
    "        \"Demographic parity difference\": demographic_parity_difference(y_test, y_pred, sensitive_features=A_str),\n",
    "        \"Equalized odds difference\": equalized_odds_difference(y_test, y_pred, sensitive_features=A_str),\n",
    "    }\n",
    "\n",
    "pd.DataFrame.from_dict(metrics_dict, orient=\"index\", columns=[\"Metrics\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uva_thesis_explainability",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
